{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af4c1247",
   "metadata": {},
   "source": [
    "### Distilling DeepSeek Coder 1.3B for the purpose of creating a student model for test case assertion generation\n",
    "\n",
    "First we install and import the needed requirements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8c1c1e02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
      "Requirement already satisfied: torch in d:\\delft\\anaconda3\\lib\\site-packages (2.7.0+cu118)\n",
      "Requirement already satisfied: torchvision in d:\\delft\\anaconda3\\lib\\site-packages (0.22.0+cu118)\n",
      "Requirement already satisfied: torchaudio in d:\\delft\\anaconda3\\lib\\site-packages (2.7.0+cu118)\n",
      "Requirement already satisfied: filelock in d:\\delft\\anaconda3\\lib\\site-packages (from torch) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in d:\\delft\\anaconda3\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in d:\\delft\\anaconda3\\lib\\site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in d:\\delft\\anaconda3\\lib\\site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in d:\\delft\\anaconda3\\lib\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in d:\\delft\\anaconda3\\lib\\site-packages (from torch) (2023.3.0)\n",
      "Requirement already satisfied: numpy in d:\\delft\\anaconda3\\lib\\site-packages (from torchvision) (1.24.3)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in d:\\delft\\anaconda3\\lib\\site-packages (from torchvision) (9.4.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in d:\\delft\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\delft\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.1)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d94e99f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing decompression of logits...\n",
      "Entry loaded successfully.\n",
      "\n",
      "Teacher prediction:\n",
      "assertNotNull(ret);\n",
      "assertEquals(Integer.valueOf(3), ret);\n",
      "\n",
      "Reference assertions:\n",
      "assertNotNull(ret);\n",
      "assertEquals(Long.class, ret.getClass());\n",
      "assertEquals(123L, ret);\n",
      "\n",
      "Successfully decompressed logits!\n",
      "Shape: torch.Size([512, 32100])\n",
      "Data type: torch.float32\n",
      "Min value: -12.0625\n",
      "Max value: 39.4688\n",
      "Sample values (first 5): [11.985416412353516, 29.16250228881836, 1.6791677474975586, -1.7562494277954102, -1.7562494277954102]\n",
      "Compression format: quantized_4bit\n",
      "Compression ratio: 59.81x\n",
      "Original size: 31.35 MB\n",
      "Compressed size: 0.52 MB\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from Data.decompression_test import decompress_tensor_optimized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06df29c",
   "metadata": {},
   "source": [
    "Let's start with understanding the data format. We have the /Data/dataset_with_predictions.jsonl file, containing the data (both input and output) for the teacher model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "783e1a93",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "NUM_LINES_TO_INSPECT = 5\n",
    "DATA_PATH = \"Data/dataset_with_predictions.jsonl\"\n",
    "\n",
    "inspected_data = []\n",
    "\n",
    "with open(DATA_PATH, 'r') as data_file:\n",
    "    for i, line_content in enumerate(data_file):\n",
    "        if i >= NUM_LINES_TO_INSPECT:\n",
    "            break\n",
    "        data = json.loads(line_content.strip())\n",
    "        inspected_data.append(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a216998",
   "metadata": {},
   "source": [
    "Now let's look closer at the parsed JSON entry:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d57a1d95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['repository', 'focal_file', 'test_method_original', 'test_method_masked', 'assertions', 'method_under_test', 'teacher_prediction', 'teacher_parsed_assertions', 'teacher_metrics', 'teacher_logits'])\n",
      "@Test\n",
      "    public void testNaturalNumber() throws Exception {\n",
      "        Object ret = reader.read(\"123\");\n",
      "\n",
      "    }\n",
      "['assertNotNull(ret);', 'assertEquals(Long.class, ret.getClass());', 'assertEquals(123L, ret);']\n",
      "assertNotNull(ret);\n",
      "assertEquals(Integer.valueOf(3), ret);\n",
      "['assertNotNull(ret);', 'assertEquals(Integer.valueOf(3), ret);']\n",
      "{'precision': 1.0, 'recall': 0.6666666666666666, 'f1': 0.8, 'accuracy': 0.6666666666666666, 'similarity': 1.0, 'exact_matches': 2, 'generated_count': 2, 'reference_count': 3}\n"
     ]
    }
   ],
   "source": [
    "print(inspected_data[0].keys())\n",
    "print(inspected_data[0][\"test_method_masked\"])\n",
    "print(inspected_data[0][\"assertions\"])\n",
    "print(inspected_data[0][\"teacher_prediction\"])\n",
    "print(inspected_data[0][\"teacher_parsed_assertions\"])\n",
    "print(inspected_data[0][\"teacher_metrics\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328d2868",
   "metadata": {},
   "source": [
    "As we can see, the data contains the repository from which the data is taken, the file that contains the class that is being tested, the test method that was written, as well as a separated version of it (a masked test and the assertions separately), the method from the original class that is being tested, the prediction of the teacher model (both as a string and as a list of assertions), some teacher metrics regarding its prediction performance and the teacher's output logits (which we will use for the loss function of the student model).\n",
    "\n",
    "Now, for every entry from the dataset, we need to construct an input for the student model that follows the same format as the input for the teacher model (as defined in DataGeneration/train_teacher_model.py). We also need to tokenize those inputs. We do this using the StudentDataset class, that will manage and tokenize the student model's input data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1c7316d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StudentDataset(Dataset):\n",
    "    \"\"\"Dataset for the student model\"\"\"\n",
    "\n",
    "    def __init__(self, data, tokenizer, max_src_length=1024, max_tgt_length=512):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_src_length = max_src_length\n",
    "        self.max_tgt_length = max_tgt_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "\n",
    "        # Construct input: We combine focal code, test method without assertions\n",
    "        input_text = f\"FOCAL CODE:\\n{item['focal_file']}\\n\\nTEST METHOD:\\n{item['test_method_masked']}\"\n",
    "\n",
    "        # Target: The assertions that need to be generated\n",
    "        target_text = \"\\n\".join(item['assertions'])\n",
    "\n",
    "        # Tokenize inputs\n",
    "        source_encoding = self.tokenizer(\n",
    "            input_text,\n",
    "            max_length=self.max_src_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        # Tokenize targets\n",
    "        target_encoding = self.tokenizer(\n",
    "            target_text,\n",
    "            max_length=self.max_tgt_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        input_ids = source_encoding[\"input_ids\"].squeeze()\n",
    "        attention_mask = source_encoding[\"attention_mask\"].squeeze()\n",
    "        labels = target_encoding[\"input_ids\"].squeeze()\n",
    "\n",
    "        # Replace padding token id with -100 so it's ignored in loss computation\n",
    "        labels[labels == self.tokenizer.pad_token_id] = -100\n",
    "        \n",
    "        decompressed_teacher_logits = decompress_tensor_optimized(item['teacher_logits'])\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"labels\": labels,\n",
    "            \"original_input\": input_text,\n",
    "            \"original_target\": target_text,\n",
    "            \"idx\": idx,\n",
    "            \"teacher_logits\": decompressed_teacher_logits,\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10022951",
   "metadata": {},
   "source": [
    "Now that we have the dataset class itself, we will also need a method to load the data that we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b96795ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(jsonl_path):\n",
    "    \"\"\"Load data from JSONL file\"\"\"\n",
    "    data = []\n",
    "    with open(jsonl_path, 'r') as f:\n",
    "        for line in f:\n",
    "            if line.strip():\n",
    "                try:\n",
    "                    data.append(json.loads(line))\n",
    "                except json.JSONDecodeError:\n",
    "                    continue\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140f02a2",
   "metadata": {},
   "source": [
    "We also need a method to train the student model:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
