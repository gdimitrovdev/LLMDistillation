{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af4c1247",
   "metadata": {},
   "source": [
    "### Distilling DeepSeek Coder 1.3B for the purpose of creating a student model for test case assertion generation\n",
    "\n",
    "First we install and import the needed requirements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8c1c1e02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
      "Requirement already satisfied: torch in d:\\delft\\anaconda3\\lib\\site-packages (2.7.0+cu118)\n",
      "Requirement already satisfied: torchvision in d:\\delft\\anaconda3\\lib\\site-packages (0.22.0+cu118)\n",
      "Requirement already satisfied: torchaudio in d:\\delft\\anaconda3\\lib\\site-packages (2.7.0+cu118)\n",
      "Requirement already satisfied: filelock in d:\\delft\\anaconda3\\lib\\site-packages (from torch) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in d:\\delft\\anaconda3\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in d:\\delft\\anaconda3\\lib\\site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in d:\\delft\\anaconda3\\lib\\site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in d:\\delft\\anaconda3\\lib\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in d:\\delft\\anaconda3\\lib\\site-packages (from torch) (2023.3.0)\n",
      "Requirement already satisfied: numpy in d:\\delft\\anaconda3\\lib\\site-packages (from torchvision) (1.24.3)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in d:\\delft\\anaconda3\\lib\\site-packages (from torchvision) (9.4.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in d:\\delft\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\delft\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.1)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "37cd6c9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==4.26.1\n",
      "  Obtaining dependency information for transformers==4.26.1 from https://files.pythonhosted.org/packages/1e/e2/60c3f4691b16d126ee9cfe28f598b13c424b60350ab339aba81aef054b8f/transformers-4.26.1-py3-none-any.whl.metadata\n",
      "  Downloading transformers-4.26.1-py3-none-any.whl.metadata (100 kB)\n",
      "     ---------------------------------------- 0.0/100.3 kB ? eta -:--:--\n",
      "     -------------------------------------- 100.3/100.3 kB 5.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: filelock in d:\\delft\\anaconda3\\lib\\site-packages (from transformers==4.26.1) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in d:\\delft\\anaconda3\\lib\\site-packages (from transformers==4.26.1) (0.31.1)\n",
      "Requirement already satisfied: numpy>=1.17 in d:\\delft\\anaconda3\\lib\\site-packages (from transformers==4.26.1) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\delft\\anaconda3\\lib\\site-packages (from transformers==4.26.1) (23.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\delft\\anaconda3\\lib\\site-packages (from transformers==4.26.1) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in d:\\delft\\anaconda3\\lib\\site-packages (from transformers==4.26.1) (2022.7.9)\n",
      "Requirement already satisfied: requests in d:\\delft\\anaconda3\\lib\\site-packages (from transformers==4.26.1) (2.31.0)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.26.1)\n",
      "  Obtaining dependency information for tokenizers!=0.11.3,<0.14,>=0.11.1 from https://files.pythonhosted.org/packages/62/41/93d3135ec30f596a71490ce11a73572190fe80e85a2aea18f116a520cc41/tokenizers-0.13.3-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading tokenizers-0.13.3-cp311-cp311-win_amd64.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in d:\\delft\\anaconda3\\lib\\site-packages (from transformers==4.26.1) (4.65.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in d:\\delft\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.26.1) (2025.3.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\delft\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.26.1) (4.12.2)\n",
      "Requirement already satisfied: colorama in d:\\delft\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers==4.26.1) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\delft\\anaconda3\\lib\\site-packages (from requests->transformers==4.26.1) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\delft\\anaconda3\\lib\\site-packages (from requests->transformers==4.26.1) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\delft\\anaconda3\\lib\\site-packages (from requests->transformers==4.26.1) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\delft\\anaconda3\\lib\\site-packages (from requests->transformers==4.26.1) (2024.2.2)\n",
      "Downloading transformers-4.26.1-py3-none-any.whl (6.3 MB)\n",
      "   ---------------------------------------- 0.0/6.3 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.3/6.3 MB 6.5 MB/s eta 0:00:01\n",
      "   --- ------------------------------------ 0.6/6.3 MB 6.1 MB/s eta 0:00:01\n",
      "   ----- ---------------------------------- 0.9/6.3 MB 6.2 MB/s eta 0:00:01\n",
      "   ------ --------------------------------- 1.0/6.3 MB 6.6 MB/s eta 0:00:01\n",
      "   ------ --------------------------------- 1.0/6.3 MB 6.6 MB/s eta 0:00:01\n",
      "   ------- -------------------------------- 1.3/6.3 MB 4.4 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 1.5/6.3 MB 4.8 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 1.9/6.3 MB 4.9 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 2.1/6.3 MB 5.1 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 2.1/6.3 MB 5.1 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 2.1/6.3 MB 5.1 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 2.1/6.3 MB 5.1 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 2.1/6.3 MB 5.1 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 2.1/6.3 MB 5.1 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 2.1/6.3 MB 5.1 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 2.3/6.3 MB 3.1 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 2.6/6.3 MB 3.2 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 2.9/6.3 MB 3.4 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 3.2/6.3 MB 3.6 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 3.5/6.3 MB 3.8 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 3.9/6.3 MB 4.0 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 4.2/6.3 MB 4.2 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 4.2/6.3 MB 4.2 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 4.2/6.3 MB 4.2 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 4.2/6.3 MB 4.2 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 4.2/6.3 MB 4.2 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 4.2/6.3 MB 4.2 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 4.2/6.3 MB 4.2 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 4.6/6.3 MB 3.4 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 5.0/6.3 MB 3.5 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 5.2/6.3 MB 3.7 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 5.6/6.3 MB 3.7 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 6.0/6.3 MB 3.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  6.3/6.3 MB 4.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.3/6.3 MB 3.9 MB/s eta 0:00:00\n",
      "Downloading tokenizers-0.13.3-cp311-cp311-win_amd64.whl (3.5 MB)\n",
      "   ---------------------------------------- 0.0/3.5 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 0.5/3.5 MB 11.3 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 1.0/3.5 MB 10.2 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 1.2/3.5 MB 8.8 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 1.8/3.5 MB 9.4 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 2.2/3.5 MB 10.0 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 2.7/3.5 MB 9.5 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 3.1/3.5 MB 9.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  3.5/3.5 MB 9.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 3.5/3.5 MB 8.9 MB/s eta 0:00:00\n",
      "Installing collected packages: tokenizers, transformers\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.21.1\n",
      "    Uninstalling tokenizers-0.21.1:\n",
      "      Successfully uninstalled tokenizers-0.21.1\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.51.3\n",
      "    Uninstalling transformers-4.51.3:\n",
      "      Successfully uninstalled transformers-4.51.3\n",
      "Successfully installed tokenizers-0.13.3 transformers-4.26.1\n"
     ]
    }
   ],
   "source": [
    "!pip3 install transformers==4.26.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d94e99f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing decompression of logits...\n",
      "Entry loaded successfully.\n",
      "\n",
      "Teacher prediction:\n",
      "assertNotNull(ret);\n",
      "assertEquals(Integer.valueOf(3), ret);\n",
      "\n",
      "Reference assertions:\n",
      "assertNotNull(ret);\n",
      "assertEquals(Long.class, ret.getClass());\n",
      "assertEquals(123L, ret);\n",
      "\n",
      "Successfully decompressed logits!\n",
      "Shape: torch.Size([512, 32100])\n",
      "Data type: torch.float32\n",
      "Min value: -12.0625\n",
      "Max value: 39.4688\n",
      "Sample values (first 5): [11.985416412353516, 29.16250228881836, 1.6791677474975586, -1.7562494277954102, -1.7562494277954102]\n",
      "Compression format: quantized_4bit\n",
      "Compression ratio: 59.81x\n",
      "Original size: 31.35 MB\n",
      "Compressed size: 0.52 MB\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import (\n",
    "    T5ForConditionalGeneration,\n",
    "    RobertaTokenizer,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "from torch.optim import AdamW\n",
    "import os\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from difflib import SequenceMatcher\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from Data.student_dataset import StudentDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06df29c",
   "metadata": {},
   "source": [
    "Let's start with understanding the data format. We have the /Data/dataset_with_predictions.jsonl file, containing the data (both input and output) for the teacher model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "783e1a93",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "NUM_LINES_TO_INSPECT = 5\n",
    "DATA_PATH = \"Data/dataset_with_predictions.jsonl\"\n",
    "\n",
    "inspected_data = []\n",
    "\n",
    "with open(DATA_PATH, 'r') as data_file:\n",
    "    for i, line_content in enumerate(data_file):\n",
    "        if i >= NUM_LINES_TO_INSPECT:\n",
    "            break\n",
    "        data = json.loads(line_content.strip())\n",
    "        inspected_data.append(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a216998",
   "metadata": {},
   "source": [
    "Now let's look closer at the parsed JSON entry:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d57a1d95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['repository', 'focal_file', 'test_method_original', 'test_method_masked', 'assertions', 'method_under_test', 'teacher_prediction', 'teacher_parsed_assertions', 'teacher_metrics', 'teacher_logits'])\n",
      "@Test\n",
      "    public void testNaturalNumber() throws Exception {\n",
      "        Object ret = reader.read(\"123\");\n",
      "\n",
      "    }\n",
      "['assertNotNull(ret);', 'assertEquals(Long.class, ret.getClass());', 'assertEquals(123L, ret);']\n",
      "assertNotNull(ret);\n",
      "assertEquals(Integer.valueOf(3), ret);\n",
      "['assertNotNull(ret);', 'assertEquals(Integer.valueOf(3), ret);']\n",
      "{'precision': 1.0, 'recall': 0.6666666666666666, 'f1': 0.8, 'accuracy': 0.6666666666666666, 'similarity': 1.0, 'exact_matches': 2, 'generated_count': 2, 'reference_count': 3}\n"
     ]
    }
   ],
   "source": [
    "print(inspected_data[0].keys())\n",
    "print(inspected_data[0][\"test_method_masked\"])\n",
    "print(inspected_data[0][\"assertions\"])\n",
    "print(inspected_data[0][\"teacher_prediction\"])\n",
    "print(inspected_data[0][\"teacher_parsed_assertions\"])\n",
    "print(inspected_data[0][\"teacher_metrics\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328d2868",
   "metadata": {},
   "source": [
    "As we can see, the data contains the repository from which the data is taken, the file that contains the class that is being tested, the test method that was written, as well as a separated version of it (a masked test and the assertions separately), the method from the original class that is being tested, the prediction of the teacher model (both as a string and as a list of assertions), some teacher metrics regarding its prediction performance and the teacher's output logits (which we will use for the loss function of the student model).\n",
    "\n",
    "Now, for every entry from the dataset, we need to construct an input for the student model that follows the same format as the input for the teacher model (as defined in DataGeneration/train_teacher_model.py). We also need to tokenize those inputs. We do this using the StudentDataset class, that will manage and tokenize the student model's input data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c7316d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class StudentDataset(Dataset):\n",
    "# This class was moved to Data/student_dataset for multithreading purposes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10022951",
   "metadata": {},
   "source": [
    "Now that we have the dataset class itself, we will also need a method to load the data that we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b96795ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(jsonl_path, max_samples=None):\n",
    "    \"\"\"Load data from JSONL file\"\"\"\n",
    "    data = []\n",
    "    counter = 0\n",
    "    with open(jsonl_path, 'r') as f:\n",
    "        for line in f:\n",
    "            if max_samples and counter >= max_samples: break\n",
    "            if line.strip():\n",
    "                try:\n",
    "                    data.append(json.loads(line))\n",
    "                    counter += 1\n",
    "                except json.JSONDecodeError:\n",
    "                    continue\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b37e36d",
   "metadata": {},
   "source": [
    "A method to evaluate the performance of the student model on a validation set is also needed, in order to judge what the best student model is that we have had so far (without using the training data, as it may overfit). In order to judge this performance, we evaluate the model against the ground truth (the assertions), instead of comparing it to the teacher model's logits. This is because our end goal is the correct assertions being generated and this is how we evaluate the model. Therefore, while our training step uses the teacher's logits (to get the behaviour of the student mimicing the behaviour of the teacher), the end output that we evaluate is against the ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e1620a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_similarity(reference, candidate):\n",
    "    \"\"\"Calculate string similarity using SequenceMatcher\"\"\"\n",
    "    return SequenceMatcher(None, reference, candidate).ratio()\n",
    "\n",
    "\n",
    "def normalize_assertion(assertion):\n",
    "    \"\"\"Normalize assertion text for more reliable comparison\"\"\"\n",
    "    # Remove whitespace\n",
    "    assertion = re.sub(r'\\s+', ' ', assertion).strip()\n",
    "\n",
    "    # Remove variable names in certain cases\n",
    "    assertion = re.sub(r'assertEquals\\(\\s*[^,]+,\\s*([^)]+)\\)', r'assertEquals(VALUE, \\1)', assertion)\n",
    "\n",
    "    # Normalize assertion method names\n",
    "    assertion = re.sub(r'assert(Equals|That|True|False)', r'assert\\1', assertion, flags=re.IGNORECASE)\n",
    "\n",
    "    return assertion\n",
    "\n",
    "\n",
    "def evaluate_assertions(generated_assertions, reference_assertions):\n",
    "    \"\"\"Evaluate the quality of generated assertions against reference assertions\"\"\"\n",
    "\n",
    "    # Parse individual assertions if provided as multiline string\n",
    "    if isinstance(generated_assertions, str):\n",
    "        # Split by semicolons or newlines\n",
    "        generated_list = re.split(r';|\\n', generated_assertions)\n",
    "        generated_list = [a.strip() + ';' for a in generated_list if a.strip()]\n",
    "    else:\n",
    "        generated_list = generated_assertions\n",
    "\n",
    "    if isinstance(reference_assertions, str):\n",
    "        reference_list = re.split(r';|\\n', reference_assertions)\n",
    "        reference_list = [a.strip() + ';' for a in reference_list if a.strip()]\n",
    "    else:\n",
    "        reference_list = reference_assertions\n",
    "\n",
    "    # Normalize assertions\n",
    "    normalized_generated = [normalize_assertion(a) for a in generated_list]\n",
    "    normalized_reference = [normalize_assertion(a) for a in reference_list]\n",
    "\n",
    "    # Calculate exact matches (accuracy)\n",
    "    exact_matches = 0\n",
    "    for gen in normalized_generated:\n",
    "        if gen in normalized_reference:\n",
    "            exact_matches += 1\n",
    "\n",
    "    # Calculate similarity scores\n",
    "    similarity_scores = []\n",
    "    for gen in normalized_generated:\n",
    "        best_sim = 0\n",
    "        for ref in normalized_reference:\n",
    "            sim = calculate_similarity(gen, ref)\n",
    "            best_sim = max(best_sim, sim)\n",
    "        similarity_scores.append(best_sim)\n",
    "\n",
    "    # Calculate metrics\n",
    "    precision = exact_matches / len(normalized_generated) if normalized_generated else 0\n",
    "    recall = exact_matches / len(normalized_reference) if normalized_reference else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    accuracy = exact_matches / max(len(normalized_generated), len(normalized_reference)) if max(\n",
    "        len(normalized_generated), len(normalized_reference)) > 0 else 0\n",
    "\n",
    "    return {\n",
    "        \"exact_matches\": exact_matches,\n",
    "        \"generated_count\": len(normalized_generated),\n",
    "        \"reference_count\": len(normalized_reference),\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "        \"accuracy\": accuracy,\n",
    "        \"similarity_score_avg\": sum(similarity_scores) / len(similarity_scores) if similarity_scores else 0,\n",
    "        \"similarity_scores\": similarity_scores\n",
    "    }\n",
    "\n",
    "\n",
    "def evaluate_model(model, tokenizer, dataloader, device):\n",
    "    \"\"\"Evaluate model on dataloader\"\"\"\n",
    "    model.eval()\n",
    "    eval_loss = 0.0\n",
    "    all_metrics = {\n",
    "        \"exact_matches\": 0,\n",
    "        \"generated_count\": 0,\n",
    "        \"reference_count\": 0,\n",
    "        \"similarity_scores\": [],\n",
    "        \"accuracy_scores\": [],\n",
    "        \"f1_scores\": []\n",
    "    }\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            # Move batch to device\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "            )\n",
    "\n",
    "            # Get loss\n",
    "            loss = outputs.loss\n",
    "            eval_loss += loss.item()\n",
    "\n",
    "            # Generate predictions for evaluation\n",
    "            generated_ids = model.generate(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                max_length=512,\n",
    "                num_beams=4,\n",
    "                early_stopping=True\n",
    "            )\n",
    "\n",
    "            # Decode and evaluate\n",
    "            for i in range(len(input_ids)):\n",
    "                generated_text = tokenizer.decode(generated_ids[i], skip_special_tokens=True)\n",
    "                reference_text = batch[\"original_target\"][i]\n",
    "\n",
    "                try:\n",
    "                    metrics = evaluate_assertions(generated_text, reference_text)\n",
    "\n",
    "                    # Update metrics\n",
    "                    all_metrics[\"exact_matches\"] += metrics[\"exact_matches\"]\n",
    "                    all_metrics[\"generated_count\"] += metrics[\"generated_count\"]\n",
    "                    all_metrics[\"reference_count\"] += metrics[\"reference_count\"]\n",
    "                    all_metrics[\"similarity_scores\"].extend(metrics[\"similarity_scores\"])\n",
    "                    all_metrics[\"accuracy_scores\"].append(metrics[\"accuracy\"])\n",
    "                    all_metrics[\"f1_scores\"].append(metrics[\"f1\"])\n",
    "                except Exception as e:\n",
    "                    print(f\"Error evaluating assertion: {e}\")\n",
    "\n",
    "    # Calculate overall metrics\n",
    "    avg_loss = eval_loss / len(dataloader)\n",
    "\n",
    "    if all_metrics[\"generated_count\"] > 0 and all_metrics[\"reference_count\"] > 0:\n",
    "        overall_precision = all_metrics[\"exact_matches\"] / all_metrics[\"generated_count\"]\n",
    "        overall_recall = all_metrics[\"exact_matches\"] / all_metrics[\"reference_count\"]\n",
    "        overall_f1 = 2 * overall_precision * overall_recall / (overall_precision + overall_recall) if (\n",
    "                                                                                                                  overall_precision + overall_recall) > 0 else 0\n",
    "        total_count = all_metrics[\"generated_count\"] + all_metrics[\"reference_count\"]\n",
    "        overall_accuracy = all_metrics[\"exact_matches\"] * 2 / total_count if total_count > 0 else 0\n",
    "    else:\n",
    "        overall_precision = 0\n",
    "        overall_recall = 0\n",
    "        overall_f1 = 0\n",
    "        overall_accuracy = 0\n",
    "\n",
    "    avg_similarity = sum(all_metrics[\"similarity_scores\"]) / len(all_metrics[\"similarity_scores\"]) if all_metrics[\n",
    "        \"similarity_scores\"] else 0\n",
    "    avg_per_sample_accuracy = sum(all_metrics[\"accuracy_scores\"]) / len(all_metrics[\"accuracy_scores\"]) if all_metrics[\n",
    "        \"accuracy_scores\"] else 0\n",
    "    avg_f1 = sum(all_metrics[\"f1_scores\"]) / len(all_metrics[\"f1_scores\"]) if all_metrics[\"f1_scores\"] else 0\n",
    "\n",
    "    eval_results = {\n",
    "        \"precision\": overall_precision,\n",
    "        \"recall\": overall_recall,\n",
    "        \"f1\": overall_f1,\n",
    "        \"accuracy\": overall_accuracy,\n",
    "        \"avg_per_sample_accuracy\": avg_per_sample_accuracy,\n",
    "        \"similarity_score_avg\": avg_similarity,\n",
    "        \"avg_per_sample_f1\": avg_f1,\n",
    "        \"total_exact_matches\": all_metrics[\"exact_matches\"],\n",
    "        \"total_generated\": all_metrics[\"generated_count\"],\n",
    "        \"total_reference\": all_metrics[\"reference_count\"]\n",
    "    }\n",
    "\n",
    "    return avg_loss, eval_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140f02a2",
   "metadata": {},
   "source": [
    "We also need a method to train the student model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "197e16a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_student_model(model, tokenizer, train_dataloader, val_dataloader, args):\n",
    "    \"\"\"Train the student model to match the teacher's output on the assertion generation task\"\"\"\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    # Setup tensorboard if available\n",
    "    try:\n",
    "        from torch.utils.tensorboard import SummaryWriter\n",
    "        tensorboard_writer = SummaryWriter(log_dir=os.path.join(args[\"output_dir\"], \"tensorboard\"))\n",
    "        use_tensorboard = True\n",
    "    except ImportError:\n",
    "        use_tensorboard = False\n",
    "\n",
    "    # Prepare optimizer and scheduler\n",
    "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_grouped_parameters = [\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "            \"weight_decay\": args[\"weight_decay\"],\n",
    "        },\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "            \"weight_decay\": 0.0,\n",
    "        },\n",
    "    ]\n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=args[\"learning_rate\"])\n",
    "\n",
    "    # Calculate total training steps\n",
    "    if args[\"max_steps\"] > 0:\n",
    "        t_total = args[\"max_steps\"]\n",
    "        num_epochs = args[\"max_steps\"] // (len(train_dataloader) // args[\"gradient_accumulation_steps\"]) + 1\n",
    "    else:\n",
    "        t_total = len(train_dataloader) // args[\"gradient_accumulation_steps\"] * args[\"epochs\"]\n",
    "        num_epochs = args[\"epochs\"]\n",
    "\n",
    "    # Create scheduler\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=args[\"warmup_steps\"],\n",
    "        num_training_steps=t_total\n",
    "    )\n",
    "\n",
    "    # Mixed precision training if requested\n",
    "    scaler = torch.cuda.amp.GradScaler() if args[\"fp16\"] else None\n",
    "\n",
    "    # Track metrics\n",
    "    best_val_loss = float('inf')\n",
    "    global_step = 0\n",
    "    epochs_without_improvement = 0\n",
    "\n",
    "    # For per-batch metrics\n",
    "    batch_loss_window_size = args[\"batch_metrics_window\"]\n",
    "    batch_losses = []\n",
    "    batch_accuracies = []\n",
    "    batch_similarities = []\n",
    "    eval_pool_size = args[\"batch_eval_pool\"]  # Number of examples to evaluate in each batch\n",
    "\n",
    "    # Create output directory\n",
    "    os.makedirs(args[\"output_dir\"], exist_ok=True)\n",
    "\n",
    "    # Save training arguments\n",
    "    with open(os.path.join(args[\"output_dir\"], \"training_args.json\"), \"w\") as f:\n",
    "        json.dump(args, f, indent=4)\n",
    "\n",
    "    # Create metrics file\n",
    "    metrics_file = os.path.join(args[\"output_dir\"], \"training_metrics.csv\")\n",
    "    with open(metrics_file, \"w\") as f:\n",
    "        f.write(\"epoch,batch,global_step,loss,accuracy,similarity,lr,examples_per_second\\n\")\n",
    "\n",
    "    # Main training loop\n",
    "    print(f\"Starting training for {num_epochs} epochs...\")\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        epoch_start_time = time.time()\n",
    "        examples_processed = 0\n",
    "\n",
    "        progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "\n",
    "        for batch_idx, batch in enumerate(progress_bar):\n",
    "            batch_start_time = time.time()\n",
    "            examples_in_batch = len(batch[\"input_ids\"])\n",
    "            examples_processed += examples_in_batch\n",
    "\n",
    "            # Move batch to device\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            teacher_logits = batch[\"teacher_logits\"].to(device)\n",
    "\n",
    "            # Forward pass with optional mixed precision\n",
    "            if args[\"fp16\"]:\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    outputs = model(\n",
    "                        input_ids=input_ids,\n",
    "                        attention_mask=attention_mask,\n",
    "                        labels=labels\n",
    "                    )\n",
    "                    loss_ce = outputs.loss / args[\"gradient_accumulation_steps\"]\n",
    "                    student_logits = outputs.logits\n",
    "                    \n",
    "                    if student_logits.shape[1] != teacher_logits.shape[1]:\n",
    "                        print(\"Student and teacher logits sizes do not match\")\n",
    "                        pass\n",
    "                    \n",
    "                    active_loss_mask = labels.view(-1) != -100 # Flattened mask\n",
    "\n",
    "                    active_student_log_probs = torch.nn.functional.log_softmax(\n",
    "                        student_logits.view(-1, student_logits.size(-1))[active_loss_mask] / args[\"distillation_temp\"],\n",
    "                        dim=-1\n",
    "                    )\n",
    "                    active_teacher_probs = torch.nn.functional.softmax(\n",
    "                        teacher_logits.view(-1, teacher_logits.size(-1))[active_loss_mask] / args[\"distillation_temp\"],\n",
    "                        dim=-1\n",
    "                    )\n",
    "\n",
    "                    if active_student_log_probs.numel() > 0:\n",
    "                        loss_fct_kl = torch.nn.KLDivLoss(reduction=\"batchmean\")\n",
    "                        loss_distill = loss_fct_kl(\n",
    "                            active_student_log_probs,\n",
    "                            active_teacher_probs\n",
    "                        ) * (args[\"distillation_temp\"] ** 2) # Scale by T^2\n",
    "                    else:\n",
    "                        loss_distill = torch.tensor(0.0, device=loss_ce.device, dtype=loss_ce.dtype)\n",
    "                        \n",
    "                    loss = args[\"alpha_ce\"] * loss_ce + args[\"alpha_distil\"] * loss_distill\n",
    "\n",
    "                # Backward pass with gradient scaling\n",
    "                scaler.scale(loss).backward()\n",
    "\n",
    "                if (batch_idx + 1) % args[\"gradient_accumulation_steps\"] == 0:\n",
    "                    # Unscales the gradients\n",
    "                    scaler.unscale_(optimizer)\n",
    "\n",
    "                    # Clip gradients\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), args[\"max_grad_norm\"])\n",
    "\n",
    "                    # Update weights\n",
    "                    scaler.step(optimizer)\n",
    "                    scheduler.step()\n",
    "                    scaler.update()\n",
    "                    optimizer.zero_grad()\n",
    "                    global_step += 1\n",
    "            else:\n",
    "                # Standard forward and backward pass\n",
    "                outputs = model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    labels=labels\n",
    "                )\n",
    "                loss_ce = outputs.loss / args[\"gradient_accumulation_steps\"]\n",
    "                student_logits = outputs.logits\n",
    "                    \n",
    "                if student_logits.shape[1] != teacher_logits.shape[1]:\n",
    "                    print(\"Student and teacher logits sizes do not match\")\n",
    "                    pass\n",
    "\n",
    "                active_loss_mask = labels.view(-1) != -100 # Flattened mask\n",
    "\n",
    "                active_student_log_probs = torch.nn.functional.log_softmax(\n",
    "                    student_logits.view(-1, student_logits.size(-1))[active_loss_mask] / args[\"distillation_temp\"],\n",
    "                    dim=-1\n",
    "                )\n",
    "                active_teacher_probs = torch.nn.functional.softmax(\n",
    "                    teacher_logits.view(-1, teacher_logits.size(-1))[active_loss_mask] / args[\"distillation_temp\"],\n",
    "                    dim=-1\n",
    "                )\n",
    "\n",
    "                if active_student_log_probs.numel() > 0:\n",
    "                    loss_fct_kl = torch.nn.KLDivLoss(reduction=\"batchmean\")\n",
    "                    loss_distill = loss_fct_kl(\n",
    "                        active_student_log_probs,\n",
    "                        active_teacher_probs\n",
    "                    ) * (args[\"distillation_temp\"] ** 2) # Scale by T^2\n",
    "                else:\n",
    "                    loss_distill = torch.tensor(0.0, device=loss_ce.device, dtype=loss_ce.dtype)\n",
    "                    \n",
    "                loss = args[\"alpha_ce\"] * loss_ce + args[\"alpha_distil\"] * loss_distill\n",
    "\n",
    "                # Backward pass\n",
    "                loss.backward()\n",
    "\n",
    "                if (batch_idx + 1) % args[\"gradient_accumulation_steps\"] == 0:\n",
    "                    # Clip gradients\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), args[\"max_grad_norm\"])\n",
    "\n",
    "                    # Update weights\n",
    "                    optimizer.step()\n",
    "                    scheduler.step()\n",
    "                    optimizer.zero_grad()\n",
    "                    global_step += 1\n",
    "\n",
    "            # Track the loss\n",
    "            loss_value = loss.item() * args[\"gradient_accumulation_steps\"]\n",
    "            epoch_loss += loss_value\n",
    "            batch_losses.append(loss_value)\n",
    "            if len(batch_losses) > batch_loss_window_size:\n",
    "                batch_losses.pop(0)\n",
    "\n",
    "            # Calculate time per example\n",
    "            batch_time = time.time() - batch_start_time\n",
    "            examples_per_second = examples_in_batch / batch_time if batch_time > 0 else 0\n",
    "\n",
    "            # Per-batch metrics: Generate predictions for a few examples to calculate accuracy\n",
    "            if args[\"track_batch_metrics\"] and batch_idx % args[\"batch_metrics_every\"] == 0:\n",
    "                # Sample some examples from batch to evaluate\n",
    "                eval_indices = np.random.choice(\n",
    "                    range(len(input_ids)),\n",
    "                    size=min(eval_pool_size, len(input_ids)),\n",
    "                    replace=False\n",
    "                )\n",
    "\n",
    "                # Switch to eval mode temporarily\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    # Generate predictions for sampled examples\n",
    "                    sampled_input_ids = input_ids[eval_indices]\n",
    "                    sampled_attention_mask = attention_mask[eval_indices]\n",
    "\n",
    "                    generated_ids = model.generate(\n",
    "                        input_ids=sampled_input_ids,\n",
    "                        attention_mask=sampled_attention_mask,\n",
    "                        max_length=args[\"max_tgt_length\"],\n",
    "                        num_beams=4,\n",
    "                        early_stopping=True\n",
    "                    )\n",
    "\n",
    "                    # Calculate accuracy and similarity\n",
    "                    batch_accuracy = 0\n",
    "                    batch_similarity = 0\n",
    "\n",
    "                    for i, idx in enumerate(eval_indices):\n",
    "                        generated_text = tokenizer.decode(generated_ids[i], skip_special_tokens=True)\n",
    "                        reference_text = batch[\"original_target\"][idx]\n",
    "\n",
    "                        try:\n",
    "                            metrics = evaluate_assertions(generated_text, reference_text)\n",
    "                            batch_accuracy += metrics[\"accuracy\"]\n",
    "                            batch_similarity += metrics[\"similarity_score_avg\"]\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error evaluating assertion: {e}\")\n",
    "\n",
    "                    # Average the metrics\n",
    "                    batch_accuracy /= len(eval_indices) if eval_indices else 1\n",
    "                    batch_similarity /= len(eval_indices) if eval_indices else 1\n",
    "\n",
    "                # Switch back to train mode\n",
    "                model.train()\n",
    "\n",
    "                # Track metrics\n",
    "                batch_accuracies.append(batch_accuracy)\n",
    "                batch_similarities.append(batch_similarity)\n",
    "                if len(batch_accuracies) > batch_loss_window_size:\n",
    "                    batch_accuracies.pop(0)\n",
    "                if len(batch_similarities) > batch_loss_window_size:\n",
    "                    batch_similarities.pop(0)\n",
    "\n",
    "                # Calculate moving averages\n",
    "                avg_loss = sum(batch_losses) / len(batch_losses)\n",
    "                avg_accuracy = sum(batch_accuracies) / len(batch_accuracies) if batch_accuracies else 0\n",
    "                avg_similarity = sum(batch_similarities) / len(batch_similarities) if batch_similarities else 0\n",
    "\n",
    "                # Update progress bar\n",
    "                progress_bar.set_postfix({\n",
    "                    \"loss\": avg_loss,\n",
    "                    \"accuracy\": avg_accuracy,\n",
    "                    \"similarity\": avg_similarity,\n",
    "                    \"ex/s\": f\"{examples_per_second:.1f}\"\n",
    "                })\n",
    "\n",
    "                # Log to tensorboard\n",
    "                if use_tensorboard:\n",
    "                    tensorboard_writer.add_scalar(\"batch_loss\", avg_loss, global_step)\n",
    "                    tensorboard_writer.add_scalar(\"batch_accuracy\", avg_accuracy, global_step)\n",
    "                    tensorboard_writer.add_scalar(\"batch_similarity\", avg_similarity, global_step)\n",
    "                    tensorboard_writer.add_scalar(\"lr\", scheduler.get_last_lr()[0], global_step)\n",
    "                    tensorboard_writer.add_scalar(\"examples_per_second\", examples_per_second, global_step)\n",
    "\n",
    "                # Log to CSV\n",
    "                with open(metrics_file, \"a\") as f:\n",
    "                    f.write(\n",
    "                        f\"{epoch + 1},{batch_idx + 1},{global_step},{avg_loss:.6f},{avg_accuracy:.6f},{avg_similarity:.6f},{scheduler.get_last_lr()[0]:.8f},{examples_per_second:.2f}\\n\")\n",
    "            else:\n",
    "                # Just update with loss\n",
    "                avg_loss = sum(batch_losses) / len(batch_losses)\n",
    "                progress_bar.set_postfix({\n",
    "                    \"loss\": avg_loss,\n",
    "                    \"ex/s\": f\"{examples_per_second:.1f}\"\n",
    "                })\n",
    "\n",
    "            # Evaluate periodically\n",
    "            if args[\"eval_steps\"] > 0 and global_step % args[\"eval_steps\"] == 0:\n",
    "                val_loss, eval_results = evaluate_model(model, tokenizer, val_dataloader, device)\n",
    "\n",
    "                # Log to tensorboard\n",
    "                if use_tensorboard:\n",
    "                    tensorboard_writer.add_scalar(\"eval_loss\", val_loss, global_step)\n",
    "                    for metric, value in eval_results.items():\n",
    "                        if isinstance(value, (int, float)):\n",
    "                            tensorboard_writer.add_scalar(f\"eval_{metric}\", value, global_step)\n",
    "\n",
    "                # Print evaluation results\n",
    "                print(f\"\\nEvaluation at step {global_step}:\")\n",
    "                print(f\"  Validation loss: {val_loss:.4f}\")\n",
    "                print(f\"  Similarity score: {eval_results['similarity_score_avg']:.4f}\")\n",
    "                print(f\"  Accuracy: {eval_results['accuracy']:.4f}\")\n",
    "                print(f\"  F1 score: {eval_results['f1']:.4f}\")\n",
    "\n",
    "                # Save best model\n",
    "                if val_loss < best_val_loss:\n",
    "                    best_val_loss = val_loss\n",
    "                    print(f\"  New best validation loss: {val_loss:.4f}\")\n",
    "\n",
    "                    # Save model checkpoint\n",
    "                    model_dir = os.path.join(args[\"output_dir\"], \"best_model\")\n",
    "                    os.makedirs(model_dir, exist_ok=True)\n",
    "                    model.save_pretrained(model_dir)\n",
    "                    tokenizer.save_pretrained(model_dir)\n",
    "\n",
    "                    # Save optimizer and scheduler\n",
    "                    torch.save(optimizer.state_dict(), os.path.join(model_dir, \"optimizer.pt\"))\n",
    "                    torch.save(scheduler.state_dict(), os.path.join(model_dir, \"scheduler.pt\"))\n",
    "\n",
    "                    # Reset patience counter\n",
    "                    epochs_without_improvement = 0\n",
    "                else:\n",
    "                    # Increment patience counter\n",
    "                    epochs_without_improvement += 1\n",
    "\n",
    "                # Early stopping\n",
    "                if 0 < args[\"early_stopping_patience\"] <= epochs_without_improvement:\n",
    "                    print(f\"Early stopping after {epochs_without_improvement} evaluations without improvement\")\n",
    "                    break\n",
    "\n",
    "                # Back to training mode\n",
    "                model.train()\n",
    "\n",
    "            # Save checkpoint\n",
    "            if args[\"save_steps\"] > 0 and global_step % args[\"save_steps\"] == 0:\n",
    "                checkpoint_dir = os.path.join(args[\"output_dir\"], f\"checkpoint-{global_step}\")\n",
    "                os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "                model.save_pretrained(checkpoint_dir)\n",
    "                tokenizer.save_pretrained(checkpoint_dir)\n",
    "\n",
    "                # Save optimizer and scheduler\n",
    "                torch.save(optimizer.state_dict(), os.path.join(checkpoint_dir, \"optimizer.pt\"))\n",
    "                torch.save(scheduler.state_dict(), os.path.join(checkpoint_dir, \"scheduler.pt\"))\n",
    "\n",
    "            # Break if max steps reached\n",
    "            if args[\"max_steps\"] > 0 and global_step >= args[\"max_steps\"]:\n",
    "                break\n",
    "\n",
    "        # Calculate average loss for the epoch\n",
    "        epoch_avg_loss = epoch_loss / len(train_dataloader)\n",
    "        train_losses.append(epoch_avg_loss)\n",
    "\n",
    "        # Calculate epoch time and speed\n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "        examples_per_second = examples_processed / epoch_time if epoch_time > 0 else 0\n",
    "\n",
    "        print(f\"\\nEpoch {epoch + 1}/{num_epochs} completed in {epoch_time:.2f}s ({examples_per_second:.2f} examples/s)\")\n",
    "        print(f\"  Average training loss: {epoch_avg_loss:.4f}\")\n",
    "\n",
    "        # Evaluate at the end of each epoch\n",
    "        print(f\"  Evaluating epoch {epoch + 1}...\")\n",
    "        val_loss, eval_results = evaluate_model(model, tokenizer, val_dataloader, device)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        # Log to tensorboard\n",
    "        if use_tensorboard:\n",
    "            tensorboard_writer.add_scalar(\"epoch_train_loss\", epoch_avg_loss, epoch + 1)\n",
    "            tensorboard_writer.add_scalar(\"epoch_val_loss\", val_loss, epoch + 1)\n",
    "            for metric, value in eval_results.items():\n",
    "                if isinstance(value, (int, float)):\n",
    "                    tensorboard_writer.add_scalar(f\"epoch_eval_{metric}\", value, epoch + 1)\n",
    "\n",
    "        # Print evaluation results\n",
    "        print(f\"  Validation loss: {val_loss:.4f}\")\n",
    "        print(f\"  Similarity score: {eval_results['similarity_score_avg']:.4f}\")\n",
    "        print(f\"  Accuracy: {eval_results['accuracy']:.4f}\")\n",
    "        print(f\"  F1 score: {eval_results['f1']:.4f}\")\n",
    "\n",
    "        # Save best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            print(f\"  New best validation loss: {val_loss:.4f}\")\n",
    "\n",
    "            # Save model checkpoint\n",
    "            model_dir = os.path.join(args[\"output_dir\"], \"best_model\")\n",
    "            os.makedirs(model_dir, exist_ok=True)\n",
    "            model.save_pretrained(model_dir)\n",
    "            tokenizer.save_pretrained(model_dir)\n",
    "\n",
    "            # Reset patience counter\n",
    "            epochs_without_improvement = 0\n",
    "        else:\n",
    "            # Increment patience counter\n",
    "            epochs_without_improvement += 1\n",
    "\n",
    "        # Early stopping\n",
    "        if 0 < args[\"early_stopping_patience\"] <= epochs_without_improvement:\n",
    "            print(f\"Early stopping after {epochs_without_improvement} epochs without improvement\")\n",
    "            break\n",
    "\n",
    "    # Save final model\n",
    "    final_model_dir = os.path.join(args[\"output_dir\"], \"final_model\")\n",
    "    os.makedirs(final_model_dir, exist_ok=True)\n",
    "    model.save_pretrained(final_model_dir)\n",
    "    tokenizer.save_pretrained(final_model_dir)\n",
    "\n",
    "    # Close tensorboard writer\n",
    "    if use_tensorboard:\n",
    "        tensorboard_writer.close()\n",
    "\n",
    "    # Plot loss curves\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(1, len(train_losses) + 1), train_losses, label='Training Loss')\n",
    "    plt.plot(range(1, len(val_losses) + 1), val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(args[\"output_dir\"], \"loss_curves.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    return model, tokenizer, best_val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ccaa13",
   "metadata": {},
   "source": [
    "Now we will also need to run the training of our student model. For this, we will need to define the training configuration and a method to run the training with that configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d3a4aa0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    # --- Data Args ---\n",
    "    \"data_path\": \"Data/dataset_with_predictions.jsonl\",\n",
    "    \"output_dir\": \"./student_model_output\",\n",
    "    \"model_name\": \"Salesforce/codet5-small\",\n",
    "    \"validation_split\": 0.1,\n",
    "    \"max_src_length\": 1024,\n",
    "    \"max_tgt_length\": 512,\n",
    "\n",
    "    # --- Distillation Args ---\n",
    "    \"distillation_temp\": 2.0,\n",
    "    \"alpha_ce\": 0.0,            # Weight for student's own Cross-Entropy loss\n",
    "    \"alpha_distil\": 1.0,        # Weight for distillation loss (e.g., KL divergence)\n",
    "\n",
    "    # --- Training Args ---\n",
    "    \"epochs\": 3,\n",
    "    \"batch_size\": 1,\n",
    "    \"eval_batch_size\": 1,\n",
    "    \"gradient_accumulation_steps\": 8,\n",
    "    \"learning_rate\": 5e-5,\n",
    "    \"weight_decay\": 0.01,\n",
    "    \"warmup_steps\": 0,\n",
    "    \"max_grad_norm\": 1.0,\n",
    "    \"max_steps\": -1,\n",
    "    \"fp16\": True,\n",
    "    \"seed\": 42,\n",
    "\n",
    "    # --- Batch Metrics Args (Optional, can simplify by removing) ---\n",
    "    \"track_batch_metrics\": False,\n",
    "    \"batch_metrics_every\": 50,\n",
    "    \"batch_metrics_window\": 50,\n",
    "    \"batch_eval_pool\": 4,\n",
    "\n",
    "    # --- Logging and Saving Args ---\n",
    "    \"logging_steps\": 100,\n",
    "    \"eval_strategy\": \"epoch\",\n",
    "    \"eval_steps\": 0,\n",
    "    \"save_strategy\": \"epoch\",\n",
    "    \"save_steps\": 0,\n",
    "    \"save_total_limit\": 2,\n",
    "    \"early_stopping_patience\": 3,\n",
    "    \"num_workers\": 2,           \n",
    "    \"max_samples\": None,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "328a9709",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_student_training(args):\n",
    "    # Set random seed\n",
    "    torch.manual_seed(args[\"seed\"])\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(args[\"seed\"])\n",
    "\n",
    "    # Load dataset\n",
    "    print(f\"Loading dataset from {args['data_path']}...\")\n",
    "    if args[\"max_samples\"] is not None:\n",
    "        data = load_dataset(args[\"data_path\"], args[\"max_samples\"])\n",
    "        print(f\"Using first {len(data)} examples\")\n",
    "    else:\n",
    "        data = load_dataset(args[\"data_path\"])\n",
    "        print(f\"Loaded {len(data)} examples\")\n",
    "\n",
    "    # Split into train and validation sets\n",
    "    train_data, val_data = train_test_split(data, test_size=args[\"validation_split\"], random_state=args[\"seed\"])\n",
    "    print(f\"Training on {len(train_data)} examples, validating on {len(val_data)} examples\")\n",
    "\n",
    "    # Load tokenizer and model\n",
    "    print(f\"Loading model: {args['model_name']}\")\n",
    "    tokenizer = RobertaTokenizer.from_pretrained(args[\"model_name\"])\n",
    "    model = T5ForConditionalGeneration.from_pretrained(args[\"model_name\"])\n",
    "\n",
    "    # Create datasets\n",
    "    train_dataset = StudentDataset(\n",
    "        train_data,\n",
    "        tokenizer,\n",
    "        max_src_length=args[\"max_src_length\"],\n",
    "        max_tgt_length=args[\"max_tgt_length\"]\n",
    "    )\n",
    "    val_dataset = StudentDataset(\n",
    "        val_data,\n",
    "        tokenizer,\n",
    "        max_src_length=args[\"max_src_length\"],\n",
    "        max_tgt_length=args[\"max_tgt_length\"]\n",
    "    )\n",
    "\n",
    "    # Create dataloaders\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=args[\"batch_size\"],\n",
    "        shuffle=True,\n",
    "        num_workers=args[\"num_workers\"],\n",
    "        pin_memory=True\n",
    "    )\n",
    "    val_dataloader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=args[\"eval_batch_size\"],\n",
    "        shuffle=False,\n",
    "        num_workers=args[\"num_workers\"],\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    # Check for CUDA\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Train model\n",
    "    model, tokenizer, best_val_loss = train_student_model(\n",
    "        model,\n",
    "        tokenizer,\n",
    "        train_dataloader,\n",
    "        val_dataloader,\n",
    "        args\n",
    "    )\n",
    "\n",
    "    print(f\"Training completed! Best validation loss: {best_val_loss:.4f}\")\n",
    "    print(f\"Trained model and checkpoints saved to {args['output_dir']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "44c7c246",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from Data/dataset_with_predictions.jsonl...\n",
      "Loaded 20000 examples\n",
      "Training on 18000 examples, validating on 2000 examples\n",
      "Loading model: Salesforce/codet5-small\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Delft\\Anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_20312\\1332176516.py:45: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler() if args[\"fp16\"] else None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training for 3 epochs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3:   0%|                                                                             | 0/18000 [00:00<?, ?it/s]C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_20312\\1332176516.py:97: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 1/3:   0%|                                             | 7/18000 [13:16<154:55:55, 31.00s/it, loss=104, ex/s=2.2]D:\\Delft\\Anaconda3\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:182: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "Epoch 1/3: 100%|██████████████████████████████████████████| 18000/18000 [2:56:27<00:00,  1.70it/s, loss=20.2, ex/s=1.9]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/3 completed in 10587.16s (1.70 examples/s)\n",
      "  Average training loss: 25.0038\n",
      "  Evaluating epoch 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|████████████████████████████████████████████████████████████████| 2000/2000 [2:02:26<00:00,  3.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Validation loss: 1.6048\n",
      "  Similarity score: 0.5811\n",
      "  Accuracy: 0.0275\n",
      "  F1 score: 0.0275\n",
      "  New best validation loss: 1.6048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3: 100%|████████████████████████████████████████████| 18000/18000 [3:13:14<00:00,  1.55it/s, loss=16, ex/s=1.9]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2/3 completed in 11594.20s (1.55 examples/s)\n",
      "  Average training loss: 17.8784\n",
      "  Evaluating epoch 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|████████████████████████████████████████████████████████████████| 2000/2000 [1:47:04<00:00,  3.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Validation loss: 1.4193\n",
      "  Similarity score: 0.6194\n",
      "  Accuracy: 0.0645\n",
      "  F1 score: 0.0645\n",
      "  New best validation loss: 1.4193\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3: 100%|██████████████████████████████████████████| 18000/18000 [3:31:48<00:00,  1.42it/s, loss=15.7, ex/s=0.6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3/3 completed in 12708.76s (1.42 examples/s)\n",
      "  Average training loss: 16.0753\n",
      "  Evaluating epoch 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|████████████████████████████████████████████████████████████████| 2000/2000 [1:39:13<00:00,  2.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Validation loss: 1.3767\n",
      "  Similarity score: 0.6120\n",
      "  Accuracy: 0.0840\n",
      "  F1 score: 0.0840\n",
      "  New best validation loss: 1.3767\n",
      "Training completed! Best validation loss: 1.3767\n",
      "Trained model and checkpoints saved to ./student_model_output\n"
     ]
    }
   ],
   "source": [
    "run_student_training(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5028f9f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0327d94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
